{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取資料\n",
    "train_df=pd.read_csv(\"C:/Users/user/Desktop/train.csv\")\n",
    "test_df=pd.read_csv(\"C:/Users/user/Desktop/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#資料前處理\n",
    "#categorical features\n",
    "categorical = train_df.select_dtypes(include =[np.object])\n",
    "print(\"Categorical Features in DataSet:\",categorical.shape[1])\n",
    "print(categorical.columns)\n",
    "#numerical features\n",
    "numerical= train_df.select_dtypes(include =[np.float64,np.int64])\n",
    "print(\"Numerical Features in DataSet:\",numerical.shape[1])\n",
    "print(numerical.columns)\n",
    "\n",
    "\n",
    "\n",
    "# let's check for missing values\n",
    "train_df.isnull().sum()\n",
    "test_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(train_df):\n",
    "    label = train_df[\"is_claim\"]\n",
    "    train_df = train_df.drop([\"policy_id\", \"is_claim\"], axis=1)\n",
    "    # numerical feature -> categorical feature (discreti)\n",
    "\n",
    "\n",
    "    # train.csv -> train, validation, test\n",
    "    # split test using index suffle or index sample\n",
    "    # suffle first\n",
    "    # index = list(range(len(train_df)))\n",
    "    index = np.arange(len(train_df))\n",
    "    random.seed(1)\n",
    "    random.shuffle(index) # inplace\n",
    "    # split by proportion 8:2\n",
    "    split = np.ceil(len(train_df)*0.8).astype(int)\n",
    "    train_x = train_df.loc[index[:split],]\n",
    "    test_x = train_df.loc[index[split:]]\n",
    "    train_y = label.loc[index[:split],]\n",
    "    test_y = label.loc[index[split:]]\n",
    "    # split validation\n",
    "\n",
    "    return train_x, test_x, train_y, test_y\n",
    "train_df = pd.read_csv(\"C:/Users/user/Desktop/train.csv\")\n",
    "train_x, test_x, train_y, test_y = preprocessing(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Task\n",
    "#i. Naïve Bayer classifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "def norm(val,mean,std):\n",
    "    pdf = 1/math.sqrt(2*math.pi*std) * math.exp(math.pow(val-mean,2)/(2*std*std))\n",
    "    return pdf\n",
    "\n",
    "def Naive_bayes(train:pd.DataFrame):\n",
    "    label = train.columns.values[-1]\n",
    "    feature = train.columns.values[:-1]\n",
    "    parameters = {}\n",
    "    label_value = train[label].unique()\n",
    "    parameters[label] = {}\n",
    "    for val in label_value:\n",
    "        D_C = len(train[train[label] == val])\n",
    "        D = len(train)\n",
    "        N = len(label_value)\n",
    "        parameters[label][val] = (D_C + 1)/N+D\n",
    "\n",
    "    for fea in feature:\n",
    "        if fea not in parameters.keys():\n",
    "            parameters[fea] = {}\n",
    "        # print(\"int\" in str(train[fea].dtype))\n",
    "        if (\"object\" in str(train[fea].dtype)) or (\"int\" in str(train[fea].dtype)):\n",
    "            feature_value = train[fea].unique()\n",
    "            N_i = len(feature_value)\n",
    "            for feature_val in feature_value:\n",
    "                parameters[fea][feature_val] = {}\n",
    "                for label_val in label_value:\n",
    "                    # print(fea,feature_val,label_val)\n",
    "                    D_ci = len(train.loc[(train[label] == label_val) & (train[fea] == feature_val)])\n",
    "                    D_c = len(train[train[label] == label_val][fea])\n",
    "                    # print((D_ci+1)/(N_i+D_c))\n",
    "                    parameters[fea][feature_val][label_val] = (D_ci+1)/(N_i+D_c)\n",
    "            #         break\n",
    "            #     break\n",
    "            # break\n",
    "        else:\n",
    "            for label_val in label_value:\n",
    "                parameters[fea][label_val] = {}\n",
    "                mean_f = train[train[label] == label_val][fea].mean()\n",
    "                std_f = train[train[label] == label_val][fea].std()\n",
    "                parameters[fea][label_val][\"mean\"] = mean_f\n",
    "                parameters[fea][label_val][\"std\"] = std_f\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def predic(test:pd.DataFrame,label,parameters:{}):\n",
    "    predic_label = []\n",
    "    feature = test.columns.values\n",
    "    # print(feature)\n",
    "    label_n = label.columns.values\n",
    "    label_values = label[str(label_n[0])].unique()\n",
    "    max_p = -999\n",
    "\n",
    "    res = \"\"\n",
    "    p = 0\n",
    "\n",
    "    for value in test.index:\n",
    "        for label_val in label_values:\n",
    "            p = parameters[str(label_n[0])][label_val]\n",
    "            for fea in feature:\n",
    "\n",
    "                if (\"object\" in str(test[fea].dtype)) or (\"int\" in str(test[fea].dtype)):\n",
    "                    p *= parameters[fea][test.loc[value,fea]][label_val]\n",
    "                else:\n",
    "                    p *= norm(test.loc[value,fea],mean = parameters[fea][label_val][\"mean\"],std = parameters[fea][label_val][\"std\"])\n",
    "\n",
    "            if p > max_p:\n",
    "                res = label_val\n",
    "                max_p = p\n",
    "            predic_label.append(res)\n",
    "\n",
    "    return  predic_label\n",
    "\n",
    "\n",
    "def train_x_train_y(data,wanna_test = False,test_num = 0.25):\n",
    "\n",
    "    data1 = data.iloc[np.random.permutation(len(data))]\n",
    "    if wanna_test :\n",
    "\n",
    "        data2 = data1[0:round((1-test_num)*len(data))]\n",
    "        train_y = data2.iloc[:,-1]\n",
    "        train_x = data2.drop(str(train_y.name), axis = 1)\n",
    "        data3 = data1[round((1-test_num)*len(data)):]\n",
    "        test_y = data3.iloc[:,-1]\n",
    "        test_x = data3.drop(str(test_y.name), axis = 1)\n",
    "\n",
    "        return train_x,train_y,test_x,test_y\n",
    "    else:\n",
    "        train_y = data1.iloc[:, -1]\n",
    "        train_x = data1.drop(str(train_y.name), axis = 1)\n",
    "        return train_x, train_y\n",
    "\n",
    "def confusion_matrix(result,test_y):\n",
    "    TP,TN,FT,FN = 0,0,0,0\n",
    "    print(result[1])\n",
    "    test_y = np.asarray(test_y)\n",
    "    print(test_y[1])\n",
    "    for i in range(len(result)):\n",
    "        if result[i] > 0 :\n",
    "            if result[i] == test_y[i]:\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if result[i] == test_y[i]:\n",
    "                FT += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "    accuracy = (TP + FT) / len(result)\n",
    "    cm = np.array([[TP,FN],[TN,FT]])\n",
    "    print(\"confusion_matrix:\")\n",
    "    print(cm)\n",
    "    print(\"accuracy:\",accuracy)\n",
    "\n",
    "# reference https://blog.csdn.net/CarryLvan/article/details/109236906\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data3 = pd.read_csv(\"train.csv\")\n",
    "    data3 = data3.drop(columns=\"policy_id\")\n",
    "    train_x, train_y, test_x, test_y = train_x_train_y(data3, wanna_test=True)\n",
    "    train_x_y = pd.concat([train_x, train_y], axis=1)\n",
    "    model_naive = Naive_bayes(train_x_y)\n",
    "    # print(model_naive[\"population_density\"][8794][0])\n",
    "    # print(type(test_y))\n",
    "    # print(test_x)\n",
    "    result = predic(pd.DataFrame(test_x),pd.DataFrame(test_y),model_naive)\n",
    "    print(result)\n",
    "    print(train_y)\n",
    "    confusion_matrix(result,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ii. Random Forest Classifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import operator\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "def transfrom_to_train(data: pd.DataFrame):\n",
    "    data_ = data.to_numpy().tolist()\n",
    "    attributeList = data.columns.values.tolist()\n",
    "\n",
    "    return data_, attributeList\n",
    "class decision_tree(object):\n",
    "\n",
    "\n",
    "    def GetClassInfo(self,Dataset):\n",
    "        ClassInfo = {}\n",
    "        for item in Dataset:\n",
    "            # check答案项目\n",
    "            if item[-1] not in ClassInfo.keys():\n",
    "                ClassInfo[item[-1]] = 1\n",
    "            else:\n",
    "                ClassInfo[item[-1]] += 1\n",
    "            classInfo = dict(sorted(ClassInfo.items(), key=operator.itemgetter(1), reverse=True))\n",
    "        return classInfo\n",
    "\n",
    "    def maxclass(self,classinfo):\n",
    "        max = list(classinfo.keys())[0]\n",
    "        return max\n",
    "\n",
    "    def compute_entropy(self,Dataset):\n",
    "        classinfo  = self.GetClassInfo(Dataset)\n",
    "        Ent = 0\n",
    "        amount = 0\n",
    "        p = []\n",
    "\n",
    "        for _,value in classinfo.items():\n",
    "            p.append(value)\n",
    "            amount += value\n",
    "        for p_k in p :\n",
    "            Ent -= p_k/amount*np.log(p_k/amount)\n",
    "\n",
    "        return Ent\n",
    "\n",
    "    def getgainNPartition(self,Dataset,featureindex,featurelist):\n",
    "        gain = self.compute_entropy(Dataset)\n",
    "        lenth_dataset = len(Dataset)\n",
    "\n",
    "        NPartition = {}\n",
    "        for dataitem in Dataset:\n",
    "\n",
    "            if dataitem[featureindex] not in NPartition.keys():\n",
    "                NPartition[dataitem[featureindex]] = [dataitem]\n",
    "            else:\n",
    "                NPartition[dataitem[featureindex]].append(dataitem)\n",
    "        # if featurelist[featureindex] == \"make\":\n",
    "        #     print(NPartition)\n",
    "        # print(NPartition)\n",
    "        lenth = []\n",
    "        Ent = []\n",
    "        amount = 0\n",
    "\n",
    "        for _,subdataset in NPartition.items():\n",
    "            Ent.append(self.compute_entropy(subdataset))\n",
    "            lenth.append(len(subdataset))\n",
    "            amount += len(subdataset)\n",
    "        for i in range(len(Ent)):\n",
    "            gain -= lenth[i]/lenth_dataset * Ent[i]\n",
    "\n",
    "        return gain,NPartition\n",
    "\n",
    "    def CreateDecisionTree(self,Dataset,featurelist):\n",
    "        lenth_dataset = len(Dataset)\n",
    "        classinfo = self.GetClassInfo(Dataset)\n",
    "        Tree = {}\n",
    "        #给定的属性集为空 ---- 不能划分\n",
    "        if len(featurelist) == 0:\n",
    "            # print(self.maxclass(classinfo))\n",
    "            return self.maxclass(classinfo)\n",
    "\n",
    "        #给定的数据集所有label都相同 ---- 无需划分\n",
    "        for key,value in classinfo.items():\n",
    "            if value == lenth_dataset:\n",
    "                return key\n",
    "                break\n",
    "        #样本在属性集上取值都相等 ---- 无法划分\n",
    "        temp = Dataset[0][:-1]\n",
    "        sment = 0\n",
    "        for dataitem in Dataset[:-1]:\n",
    "            if dataitem == temp:\n",
    "                sment += 1\n",
    "        if sment == lenth_dataset:\n",
    "\n",
    "            return self.maxclass(classinfo)\n",
    "        # 选择最佳划分属性\n",
    "        bestIdex = 0\n",
    "        bestGain = 0\n",
    "        bestNPartition = {}\n",
    "\n",
    "        for indedx in range(len(featurelist)):\n",
    "            gain,NPartition = self.getgainNPartition(Dataset,indedx,featurelist)\n",
    "            if gain > bestGain:\n",
    "                bestIdex = indedx\n",
    "                bestGain = gain\n",
    "                bestNPartition = NPartition\n",
    "\n",
    "        attr_Name = featurelist[bestIdex]\n",
    "        # print(type(featurelist))\n",
    "        del featurelist[bestIdex]\n",
    "\n",
    "        for _,vallist in bestNPartition.items():\n",
    "            for i in range(len(vallist)):\n",
    "                temp = vallist[i][:bestIdex]\n",
    "                temp.extend(vallist[i][bestIdex+1:])\n",
    "                vallist[i] = temp\n",
    "        # # 为了方便后面建子树，将此时的attr对应的那列去除\n",
    "        # 根据属性的值，建立分叉节点\n",
    "        # if bestNPartition.values() == []:\n",
    "        #     break\n",
    "\n",
    "        Tree[attr_Name] = {}\n",
    "        if not bestNPartition:\n",
    "            # print(bestNPartition.values())\n",
    "            return self.maxclass(classinfo)\n",
    "        # print(featurelist)\n",
    "        for keyAttrVal, valDataset in bestNPartition.items():\n",
    "\n",
    "            # 因为python对iterable list对象的传参是按地址传参，会改变attributeList的值\n",
    "            # 所以在传attributeList参数的时候，创建一个副本，就相当于按值传递了\n",
    "            subLabels = featurelist[:]\n",
    "\n",
    "            # valDataset是已去除attr的data，attributeList是已去除attr的attributeList\n",
    "            Tree[attr_Name][keyAttrVal] = self.CreateDecisionTree(valDataset, subLabels)\n",
    "        return Tree\n",
    "\n",
    "    def vote(self,predic_label):\n",
    "        vote_dict = {}\n",
    "        for result in predic_label:\n",
    "\n",
    "            if result not in vote_dict.keys():\n",
    "                vote_dict[result] = 1\n",
    "            else:\n",
    "                vote_dict[result] += 1\n",
    "\n",
    "            sort_vote_disc = sorted(vote_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            return sort_vote_disc[0][0]\n",
    "\n",
    "\n",
    "\n",
    "    def Predict(self,DataSet, testArrtList, decisionTree):\n",
    "        predicted_label = []\n",
    "\n",
    "        for dataItem in DataSet:\n",
    "            cur_decisionTree = decisionTree\n",
    "            # 如果root就是叶子结点leaf\n",
    "            if type(cur_decisionTree) == set:  # 例如：{'N'}\n",
    "                node = list(cur_decisionTree)\n",
    "            else:\n",
    "                node = list(cur_decisionTree.keys())[0]\n",
    "                # 只要temp处在attributeList，说明当前处在树枝结点(非叶子)上, 否则处在叶子结点\n",
    "                while node in testArrtList:\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        cur_index = testArrtList.index(node)  # 0 2\n",
    "                        cur_element = dataItem[cur_index]  # 0 0\n",
    "                        cur_decisionTree = cur_decisionTree[node][cur_element]  # {'student': {0: 'N', 1: 'Y'}} N\n",
    "                        # print(cur_decisionTree)\n",
    "                    except:\n",
    "\n",
    "                        root = list(cur_decisionTree[node].keys())\n",
    "                        # print(root)\n",
    "                        cur_element = self.vote(root)\n",
    "                        cur_decisionTree = cur_decisionTree[node][cur_element]\n",
    "\n",
    "\n",
    "                    if type(cur_decisionTree) == dict:\n",
    "                        node = list(cur_decisionTree.keys())[0]  # student\n",
    "                    else:\n",
    "                        node = cur_decisionTree\n",
    "\n",
    "\n",
    "            # print(node)\n",
    "            predicted_label.append(node)\n",
    "        return predicted_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class random_forest():\n",
    "\n",
    "    def bagging(self,dataset,sample_num,dataset_size,typ = \"None\"):\n",
    "\n",
    "        subdata_list = []\n",
    "        if typ == \"None\":\n",
    "            for datalist in range(sample_num):\n",
    "                sub = random.sample(dataset,dataset_size)\n",
    "                print(len(sub[0]),len(sub))\n",
    "                # print(sub[0][5])\n",
    "                subdata_list.append(sub)\n",
    "            return subdata_list\n",
    "        else :\n",
    "            if typ == \"time_series\":\n",
    "                pass\n",
    "\n",
    "    def vote(self,predic_label):\n",
    "        vote_dict = {}\n",
    "        for result in predic_label:\n",
    "            if result not in vote_dict.keys():\n",
    "                vote_dict[result] = 1\n",
    "            else:\n",
    "                vote_dict[result] += 1\n",
    "        sort_vote_disc = sorted(vote_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        return sort_vote_disc[0][0]\n",
    "\n",
    "    def fit(self,dataset,featurelist,num = 100,dataset_size = 500,typ = \"None\"):\n",
    "        self.model_list = []\n",
    "        # tree = decision_tree()\n",
    "        bagging_dataset = self.bagging(dataset,num,dataset_size,typ)\n",
    "        for data in bagging_dataset:\n",
    "            tree = decision_tree()\n",
    "            model = tree.CreateDecisionTree(data,featurelist)\n",
    "            self.model_list.append(model)\n",
    "\n",
    "\n",
    "    def predic(self,test_data):\n",
    "        predic_label = []\n",
    "        predic_result =[]\n",
    "        predic_dict = []\n",
    "        test_,attrib = transfrom_to_train(test_data)\n",
    "        # print(attrib)\n",
    "        for model in self.model_list:\n",
    "            # print(model.items())\n",
    "\n",
    "            label = decision_tree().Predict(test_,attrib,model)\n",
    "            # print(label)\n",
    "            predic_label.append(label)\n",
    "        # print(predic_label)\n",
    "        # print(len(predic_label[0]),len(predic_label))\n",
    "        for j in range(len(predic_label[0])):\n",
    "            for i in range(len(predic_label)):\n",
    "                # print(i,j)\n",
    "                predic_dict.append(predic_label[i][j])\n",
    "            predic_result.append(self.vote(predic_dict))\n",
    "\n",
    "        return predic_result\n",
    "\n",
    "\n",
    "def train_x_train_y(data,wanna_test = False,test_num = 0.25):\n",
    "\n",
    "    data1 = data.iloc[np.random.permutation(len(data))]\n",
    "    if wanna_test :\n",
    "\n",
    "        data2 = data1[0:round((1-test_num)*len(data))]\n",
    "        train_y = data2.iloc[:,-1]\n",
    "        train_x = data2.drop(str(train_y.name), axis = 1)\n",
    "        data3 = data1[round((1-test_num)*len(data)):]\n",
    "        test_y = data3.iloc[:,-1]\n",
    "        test_x = data3.drop(str(test_y.name), axis = 1)\n",
    "\n",
    "        return train_x,train_y,test_x,test_y\n",
    "    else:\n",
    "        train_y = data1.iloc[:, -1]\n",
    "        train_x = data1.drop(str(train_y.name), axis = 1)\n",
    "        return train_x, train_y\n",
    "\n",
    "def confusion_matrix(result,test_y):\n",
    "    TP,TN,FT,FN = 0,0,0,0\n",
    "    # print(result[1])\n",
    "    test_y = np.asarray(test_y)\n",
    "    # print(test_y[1])\n",
    "    for i in range(len(result)):\n",
    "        if result[i] > 0 :\n",
    "            if result[i] == test_y[i] :\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if result[i] == test_y[i] :\n",
    "                FT += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "    accuracy = (TP + FT) / len(result)\n",
    "    cm = np.array([[TP,FN],[TN,FT]])\n",
    "    print(\"confusion_matrix:\")\n",
    "    print(cm)\n",
    "    print(\"accuracy:\",accuracy)\n",
    "\n",
    "#referrance https://www.bilibili.com/video/BV1MK4y1P7TB/\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data3 = pd.read_csv(\"train.csv\")\n",
    "    # print(data3.shape)\n",
    "    # data3 = data3.drop(columns=[\"policy_id\"])\n",
    "    # data3 = data3.drop(columns=['policy_tenure'])\n",
    "    train_x, train_y, test_x, test_y = train_x_train_y(data3, wanna_test=True)\n",
    "    train_x_y = pd.concat([train_x, train_y], axis=1)\n",
    "    train_,attributeList = transfrom_to_train(data=train_x_y)\n",
    "    # print(attributeList)\n",
    "    del attributeList[len(attributeList)-1]\n",
    "    print(attributeList)\n",
    "    randomforest = random_forest()\n",
    "    model_forest = randomforest.fit(train_,attributeList,num=5,dataset_size = 10000)\n",
    "    result = randomforest.predic(test_x)\n",
    "    print(result)\n",
    "    confusion_matrix(result,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iii. Random Forest Classifier(scikit-learning)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def train_x_train_y(data,wanna_test = False,test_num = 0.25):\n",
    "\n",
    "    data1 = data.iloc[np.random.permutation(len(data))]\n",
    "    if wanna_test :\n",
    "\n",
    "        data2 = data1[0:round((1-test_num)*len(data))]\n",
    "        train_y = data2.iloc[:,-1]\n",
    "        train_x = data2.drop(str(train_y.name), axis = 1)\n",
    "        data3 = data1[round((1-test_num)*len(data)):]\n",
    "        test_y = data3.iloc[:,-1]\n",
    "        test_x = data3.drop(str(test_y.name), axis = 1)\n",
    "\n",
    "        return train_x,train_y,test_x,test_y\n",
    "    else:\n",
    "        train_y = data1.iloc[:, -1]\n",
    "        train_x = data1.drop(str(train_y.name), axis = 1)\n",
    "        return train_x, train_y\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "data = data.drop(columns=\"policy_id\")\n",
    "train_x,train_y,test_x,test_y  = train_x_train_y(data,wanna_test = True,test_num = 0.25)\n",
    "train_x,train_y,test_x,test_y = train_x.to_numpy(),train_y.to_numpy(),test_x.to_numpy(),test_y.to_numpy()\n",
    "# print(train_x.shape)\n",
    "# train_x = train_x.reshape((41014,-1))\n",
    "# print(train_x.shape)\n",
    "# train_y = train_y.reshape((-1,1))\n",
    "print(train_y.shape)\n",
    "# train_y = np.reshape(arousal_lable,(624,1))\n",
    "rfc = RandomForestClassifier(random_state=0,class_weight=\"balanced\")\n",
    "rfc = rfc.fit(train_x,train_y)\n",
    "score_r = rfc.score(test_x,test_y)\n",
    "pred = rfc.predict(test_x)\n",
    "print(\"Random Forest:{}\".format(score_r))\n",
    "\n",
    "mat = confusion_matrix(test_y, pred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iv. XGBoost, Catboost, LightGBM (can use other public code sources).\n",
    "#——————————————————— xgboost ———————————————————\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "data = data.drop(columns=\"policy_id\")\n",
    "\n",
    "y = data.iloc[:,-1]\n",
    "X = data.drop(str(y.name), axis = 1)\n",
    "\n",
    "x_train, x_predict, y_train, y_predict = train_test_split(X, y, test_size=0.10, random_state=100)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=100)\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(data=x_train,label=y_train,missing=-999.0)\n",
    "dtest = xgb.DMatrix(data=x_test,label=y_test,missing=-999.0)\n",
    "\n",
    "param = {'max_depth': 7, 'eta': 1, 'silent': 1, 'objective': 'reg:linear'}\n",
    "param['nthread'] = 4\n",
    "param['seed'] = 100\n",
    "param['eval_metric'] = 'auc'\n",
    "\n",
    "\n",
    "num_round = 10\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "\n",
    "bst_with_evallist_and_early_stopping_10 = xgb.train(param, dtrain, num_round*100, evallist,early_stopping_rounds=10)\n",
    "dpredict = xgb.DMatrix(x_predict)\n",
    "ypred_with_evallist_and_early_stopping_100 = bst_with_evallist_and_early_stopping_10.predict(dpredict,ntree_limit=bst_with_evallist_and_early_stopping_10.best_ntree_limit)\n",
    "print(ypred_with_evallist_and_early_stopping_100)\n",
    "\n",
    "print(\"RMSE of bst_with_evallist_and_early_stopping_100 ：\", np.sqrt(mean_squared_error(y_true=y_predict,y_pred=ypred_with_evallist_and_early_stopping_100)))\n",
    "\n",
    "\n",
    "# —————————————————— catboost ——————————————————————\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier,Pool\n",
    "\n",
    "cat_features = [0, 1]  \n",
    "\n",
    "x_train, x_predict, y_train, y_predict = train_test_split(X, y, test_size=0.10, random_state=100)\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "# 定義模型定義模型\n",
    "train_data = Pool(x_train,y_train)\n",
    "model = CatBoostClassifier(iterations=10, learning_rate=1, depth=2)\n",
    "\n",
    "# 訓練\n",
    "model.fit(train_data)\n",
    "\n",
    "# 預測類別\n",
    "preds_class = model.predict(x_predict)\n",
    "print(preds_class)\n",
    "\n",
    "# —————————————————— LightGBM ——————————————————————\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "train_data = lgb.Dataset(data=x_train,label=y_train)\n",
    "test_data = lgb.Dataset(data=x_test,label=y_test)\n",
    "\n",
    "param = {'num_leaves':31, 'num_trees':100, 'objective':'regression'}\n",
    "param['metric'] = 'rmse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation\n",
    "#i. Use k=3,5,10, and make some discussions of your observation.\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from Naivebeyes import Naive_bayes,train_x_train_y,confusion_matrix\n",
    "from randomforest import random_forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier,Pool\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "class K_fold_cross_validation(object):\n",
    "\n",
    "    def __init__(self,k = 5,dataset = \"None\",model = \"None\"):\n",
    "        \"\"\"\n",
    "        :type fit: class : fit\n",
    "        :type predic: class : predic\n",
    "        :type dataset: pd.Datafram\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.data = dataset\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "    def Average(self,lst):\n",
    "        return sum(lst) / len(lst)\n",
    "\n",
    "    def cut_data(self):\n",
    "        df = self.data\n",
    "        k = self.k\n",
    "        df_num = len(df)\n",
    "        df = df.iloc[np.random.permutation(len(df))]\n",
    "        every_epoch_num = int((df_num / k))\n",
    "        self.cut_reslut = []\n",
    "        for index in (range(k)):#tqdm\n",
    "            if index < k - 1:\n",
    "                df_tem = df[every_epoch_num * index: every_epoch_num * (index + 1)]\n",
    "            else:\n",
    "                df_tem = df[every_epoch_num * index:]\n",
    "            self.cut_reslut.append(df_tem)\n",
    "        return self.cut_reslut\n",
    "        # 原文链接：https: // blog.csdn.net / weixin_42599499 / article / details / 117809308\n",
    "    def validation(self,need_label = False,is_xgb = False,is_lg = False,is_cat = False):\n",
    "        self.cut_data()\n",
    "        mean_accuracy, mean_precision, mean_recall, mean_F1 = [],[],[],[]\n",
    "        for item in (range(self.k)):#tqdm\n",
    "            datalist = self.cut_reslut\n",
    "            vali_data = datalist[item]\n",
    "\n",
    "            traindata = pd.DataFrame()\n",
    "            for index in range(len(datalist)):\n",
    "                if index == item:\n",
    "                    continue\n",
    "                else:\n",
    "                    traindata = traindata.append(datalist[index])\n",
    "            vali_y = vali_data.iloc[:, -1]\n",
    "            vali_x = vali_data.drop(str(vali_y.name), axis=1)\n",
    "\n",
    "            train_y = traindata.iloc[:, -1]\n",
    "            train_x = traindata.drop(str(vali_y.name), axis=1)\n",
    "\n",
    "            if is_xgb:\n",
    "                param = {'max_depth': 7, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic'}\n",
    "                param['nthread'] = 4\n",
    "                param['seed'] = 100\n",
    "                param['eval_metric'] = 'auc'\n",
    "                dtrain = self.model.DMatrix(data=train_x, label=train_y, missing=-999.0)\n",
    "                dpredict = self.model.DMatrix(vali_x)\n",
    "                # evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "                self.model.train(param, dtrain, 1000,early_stopping_rounds=10)\n",
    "                result = self.model.predict(dpredict)\n",
    "            else:\n",
    "                if is_lg:\n",
    "                    train_data = self.model.Dataset(data=train_x, label=train_y)\n",
    "                    test_data = self.model.Dataset(data=vali_x, label=vali_y)\n",
    "\n",
    "                    param = {'num_leaves': 31, 'num_trees': 100, 'objective': 'binary'}\n",
    "                    param['metric'] = 'rmse'\n",
    "\n",
    "                    bst = self.model.train(param, train_data, 1000, valid_sets=[test_data])\n",
    "                    result = bst.predict(test_data, num_iteration=bst.best_iteration)\n",
    "\n",
    "                else:\n",
    "                    if is_cat:\n",
    "                        train_data = Pool(train_x, train_y)\n",
    "                        self.model.fit(train_data)\n",
    "                        result = self.model.predic(vali_x)\n",
    "                    else:\n",
    "                        self.model.fit(traindata)\n",
    "                        if need_label:\n",
    "\n",
    "                            result = self.model.predic(pd.DataFrame(vali_x), pd.DataFrame(vali_y))\n",
    "                        else:\n",
    "                            result = self.model.predic(vali_x)\n",
    "\n",
    "            a1,a2,a3,a4 = confusion_matrix(result,vali_y,if_print=False)\n",
    "            mean_accuracy.append(a1)\n",
    "            mean_precision.append(a2)\n",
    "            mean_recall.append(a3)\n",
    "            mean_F1.append(a4)\n",
    "        print('mean_accuracy: %s, mean_precision: %s, mean_recall: %s,mean_F1: %s' % (self.Average(mean_accuracy),self.Average(mean_precision),self.Average(mean_recall),self.Average(mean_F1)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def plot(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data3 = pd.read_csv(\"train.csv\")\n",
    "    data3 = data3.drop(columns=\"policy_id\")\n",
    "    train_x, train_y, test_x, test_y = train_x_train_y(data3, wanna_test=True)\n",
    "    train_x_y = pd.concat([train_x, train_y], axis=1)\n",
    "    for k in [3,5,10]:\n",
    "        # ------------- Naive bayes -------------\n",
    "        # NB = Naive_bayes()\n",
    "        # cv_N = K_fold_cross_validation(k=k, dataset = train_x_y,model=NB)\n",
    "        # cv_N.validation(need_label=True)\n",
    "        #------------- random forest -------------\n",
    "        rfc = random_forest()\n",
    "        cv_R = K_fold_cross_validation(k=k, dataset = train_x_y,model=rfc)\n",
    "        cv_R.validation()\n",
    "        #------------- sklearn random forest-------------\n",
    "        srfc = RandomForestClassifier(random_state=0, class_weight=\"balanced\")\n",
    "        cv_SR = K_fold_cross_validation(k=k, dataset=train_x_y, model=srfc)\n",
    "        cv_SR.validation()\n",
    "        # ------------- XGboost -------------\n",
    "        # model_XG =\n",
    "        cv_XG = K_fold_cross_validation(k=k, dataset=train_x_y, model=xgb)\n",
    "        cv_XG.validation(is_xgb=True)\n",
    "        # ------------- catboost -------------\n",
    "        model_cat = CatBoostClassifier(iterations=10, learning_rate=1, depth=2)\n",
    "        cv_CAT = K_fold_cross_validation(k=k, dataset=train_x_y, model=model_cat)\n",
    "        cv_CAT.validation(is_pool=True)\n",
    "        #------------- LightGBM -------------\n",
    "        # model_LG = lgb()\n",
    "        cv_LG = K_fold_cross_validation(k=k, dataset=train_x_y, model=lgb)\n",
    "        cv_XG.validation(is_lg=True)\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ii. Now you have a test dataset you have partitioned from train.csv. \n",
    "# Please design an algorithm that can merge/ aggregate the predicted resultsfrom k classifiers in k-fold cross-validation. Compare the performance and complexity of the cross-validation with Problem 1. \n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from Naivebeyes import Naive_bayes,train_x_train_y,confusion_matrix\n",
    "from randomforest import random_forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier,Pool\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "class K_fold_cross_validation(object):\n",
    "\n",
    "    def __init__(self,k = 5,dataset = \"None\",model = \"None\"):\n",
    "        \"\"\"\n",
    "        :type fit: class : fit\n",
    "        :type predic: class : predic\n",
    "        :type dataset: pd.Datafram\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.data = dataset\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "    def Average(self,lst):\n",
    "        return sum(lst) / len(lst)\n",
    "\n",
    "    def cut_data(self):\n",
    "        df = self.data\n",
    "        k = self.k\n",
    "        df_num = len(df)\n",
    "        df = df.iloc[np.random.permutation(len(df))]\n",
    "        every_epoch_num = int((df_num / k))\n",
    "        self.cut_reslut = []\n",
    "        for index in (range(k)):#tqdm\n",
    "            if index < k - 1:\n",
    "                df_tem = df[every_epoch_num * index: every_epoch_num * (index + 1)]\n",
    "            else:\n",
    "                df_tem = df[every_epoch_num * index:]\n",
    "            self.cut_reslut.append(df_tem)\n",
    "        return self.cut_reslut\n",
    "        # 原文链接：https: // blog.csdn.net / weixin_42599499 / article / details / 117809308\n",
    "    def validation(self,test_x,test_y,need_label = False,is_xgb = False,is_lg = False,is_cat = False):\n",
    "        self.cut_data()\n",
    "        mean_accuracy, mean_precision, mean_recall, mean_F1 = [],[],[],[]\n",
    "        for item in (range(self.k)):#tqdm\n",
    "            datalist = self.cut_reslut\n",
    "            vali_data = datalist[item]\n",
    "\n",
    "            traindata = pd.DataFrame()\n",
    "            for index in range(len(datalist)):\n",
    "                if index == item:\n",
    "                    continue\n",
    "                else:\n",
    "                    traindata = traindata.append(datalist[index])\n",
    "            # vali_y = vali_data.iloc[:, -1]\n",
    "            # vali_x = vali_data.drop(str(vali_y.name), axis=1)\n",
    "\n",
    "            train_y = traindata.iloc[:, -1]\n",
    "            train_x = traindata.drop(str(train_y.name), axis=1)\n",
    "\n",
    "            if is_xgb:\n",
    "                param = {'max_depth': 7, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic'}\n",
    "                param['nthread'] = 4\n",
    "                param['seed'] = 100\n",
    "                param['eval_metric'] = 'auc'\n",
    "                dtrain = self.model.DMatrix(data=train_x, label=train_y, missing=-999.0)\n",
    "                dpredict = self.model.DMatrix(test_x)\n",
    "                # evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "                xg = self.model.train(param, dtrain, 1000,early_stopping_rounds=10)\n",
    "                result = xg.predict(dpredict)\n",
    "            else:\n",
    "                if is_lg:\n",
    "                    train_data = self.model.Dataset(data=train_x, label=train_y)\n",
    "                    test_data = self.model.Dataset(data=test_x, label=test_y)\n",
    "\n",
    "                    param = {'num_leaves': 31, 'num_trees': 100, 'objective': 'binary'}\n",
    "                    param['metric'] = 'rmse'\n",
    "\n",
    "                    bst = self.model.train(param, train_data, 1000, valid_sets=[test_data])\n",
    "                    result = bst.predict(test_data, num_iteration=bst.best_iteration)\n",
    "\n",
    "                else:\n",
    "                    if is_cat:\n",
    "                        train_data = Pool(train_x, train_y)\n",
    "                        self.model.fit(train_data)\n",
    "                        result = self.model.predic(test_x)\n",
    "                    else:\n",
    "                        self.model.fit(traindata)\n",
    "                        if need_label:\n",
    "\n",
    "                            result = self.model.predic(pd.DataFrame(test_x), pd.DataFrame(test_y))\n",
    "                        else:\n",
    "                            result = self.model.predic(test_x)\n",
    "\n",
    "            a1,a2,a3,a4 = confusion_matrix(result,test_y,if_print=False)\n",
    "            mean_accuracy.append(a1)\n",
    "            mean_precision.append(a2)\n",
    "            mean_recall.append(a3)\n",
    "            mean_F1.append(a4)\n",
    "        print('mean_accuracy: %s, mean_precision: %s, mean_recall: %s,mean_F1: %s' % (self.Average(mean_accuracy),self.Average(mean_precision),self.Average(mean_recall),self.Average(mean_F1)))\n",
    "if __name__ == '__main__':\n",
    "    data3 = pd.read_csv(\"train.csv\")\n",
    "    data3 = data3.drop(columns=\"policy_id\")\n",
    "    train_x, train_y, test_x, test_y = train_x_train_y(data3, wanna_test=True)\n",
    "    train_x_y = pd.concat([train_x, train_y], axis=1)\n",
    "    for k in [5]:\n",
    "        # ------------- Naive bayes -------------\n",
    "        NB = Naive_bayes()\n",
    "        cv_N = K_fold_cross_validation(k=k, dataset = train_x_y,model=NB)\n",
    "        cv_N.validation(test_x,test_y,need_label=True)\n",
    "        #------------- random forest -------------\n",
    "        rfc = random_forest()\n",
    "        cv_R = K_fold_cross_validation(k=k, dataset = train_x_y,model=rfc)\n",
    "        cv_R.validation(test_x,test_y)\n",
    "        #------------- sklearn random forest-------------\n",
    "        srfc = RandomForestClassifier(random_state=0, class_weight=\"balanced\")\n",
    "        cv_SR = K_fold_cross_validation(k=k, dataset=train_x_y, model=srfc)\n",
    "        cv_SR.validation(test_x,test_y)\n",
    "        # ------------- XGboost -------------\n",
    "        # model_XG =\n",
    "        cv_XG = K_fold_cross_validation(k=k, dataset=train_x_y, model=xgb)\n",
    "        cv_XG.validation(test_x,test_y,is_xgb=True)\n",
    "        # ------------- catboost -------------\n",
    "        model_cat = CatBoostClassifier(iterations=10, learning_rate=1, depth=2)\n",
    "        cv_CAT = K_fold_cross_validation(k=k, dataset=train_x_y, model=model_cat)\n",
    "        cv_CAT.validation(test_x,test_y,is_cat=True)\n",
    "        #------------- LightGBM -------------\n",
    "        # model_LG = lgb()\n",
    "        cv_LG = K_fold_cross_validation(k=k, dataset=train_x_y, model=lgb)\n",
    "        cv_XG.validation(test_x,test_y,is_lg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iii. How do we know the performance of one model is really better than another one? Please compare the result in 5-fold cross-validation and the result of Problem 1 to justify which is “REALLY” better. Also show me why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22ac92fa7380955e9d5490c736288b96d340f6eeb2db5956448ab0593e5495af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
